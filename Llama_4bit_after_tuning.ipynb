{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "af6ecfa0d3d847f79e2fc4b8515ac38e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e5baddd372b4e6981eb65432fae708a",
              "IPY_MODEL_7240f801b5ed4a968d4859c55358c66b",
              "IPY_MODEL_42df532d14204db0a47571356349b100"
            ],
            "layout": "IPY_MODEL_1550be466e9e4802b3054ae7b321797a"
          }
        },
        "4e5baddd372b4e6981eb65432fae708a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b5b04b10ba24f388acffbc5bd947839",
            "placeholder": "​",
            "style": "IPY_MODEL_8304a20af94e47019966943896de0f0f",
            "value": "model.safetensors:  97%"
          }
        },
        "7240f801b5ed4a968d4859c55358c66b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2074eee3290d41ba8b6c70befad0cbf1",
            "max": 2242762780,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f1903ba1b0f4203a2c7c81d11207430",
            "value": 2175654886
          }
        },
        "42df532d14204db0a47571356349b100": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad370236f51140299bace6801efdb976",
            "placeholder": "​",
            "style": "IPY_MODEL_3d01d1489f4e4c13860ccbd4d7a71901",
            "value": " 2.18G/2.24G [00:44&lt;00:02, 22.9MB/s]"
          }
        },
        "1550be466e9e4802b3054ae7b321797a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b5b04b10ba24f388acffbc5bd947839": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8304a20af94e47019966943896de0f0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2074eee3290d41ba8b6c70befad0cbf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f1903ba1b0f4203a2c7c81d11207430": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ad370236f51140299bace6801efdb976": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d01d1489f4e4c13860ccbd4d7a71901": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThiruvothurGanesanSiva/TestingAI/blob/main/Llama_4bit_after_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "================================================================================\n",
        "📋 WORKSHOP: Emotion Classification Fine-Tuning with QLoRA\n",
        "================================================================================\n",
        "\n",
        "📋 PURPOSE:\n",
        "This notebook fine-tunes Llama 3.2 3B on the emotion classification task using\n",
        "QLoRA (Quantized Low-Rank Adaptation) for ultra-efficient training. After\n",
        "training, we test on the SAME sentences from the baseline to measure improvement.\n",
        "\n",
        "🎯 KEY CONCEPT:\n",
        "We're training the model to classify emotions into 6 categories using only\n",
        "1,000 examples. QLoRA combines 4-bit quantization + LoRA adapters to train\n",
        "efficiently on consumer GPUs without modifying the entire 3B parameter model.\n",
        "\n",
        "🎯 LEARNING OBJECTIVES:\n",
        "- Understand QLoRA: 4-bit quantized base model + LoRA adapters\n",
        "- Apply LoRA adapters while keeping base model frozen in 4-bit\n",
        "- Fine-tune on emotion dataset with proper formatting\n",
        "- Use SFTTrainer for supervised fine-tuning\n",
        "- Compare before/after results on same test cases\n",
        "- Save and load fine-tuned adapters\n",
        "\n",
        "⚙️ REQUIREMENTS:\n",
        "- Google Colab with GPU (T4 recommended, 15GB VRAM)\n",
        "- ~15-20 minutes runtime (including training)\n",
        "- Run baseline test first (llama4bit_pretraining.py) for comparison\n",
        "\n",
        "🔬 WHAT THIS DEMONSTRATES:\n",
        "- QLoRA training: 4-bit base model + rank 32 LoRA (only 0.5% params trained)\n",
        "- Extreme memory efficiency: ~2GB total (vs ~6GB for regular LoRA)\n",
        "- Dramatic improvement from baseline (poor) to fine-tuned (80-90%+ accuracy)\n",
        "- Production-ready workflow: load 4bit → add LoRA → train → test → save\n",
        "\n",
        "📚 REFERENCE:\n",
        "QLoRA paper by Tim Dettmers et al. (2023): \"QLoRA: Efficient Finetuning of\n",
        "Quantized LLMs\" - enables training 65B models on single 48GB GPU\n",
        "\n",
        "================================================================================"
      ],
      "metadata": {
        "id": "7y-v0lWmCB3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# 🔧 STEP 1: INSTALLATION\n",
        "#============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"📦 Installing Unsloth and Dependencies for Fine-Tuning\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    # Local installation (simpler)\n",
        "    !uv pip install unsloth\n",
        "else:\n",
        "    # Colab installation (optimized for Colab environment)\n",
        "    !uv pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "    !uv pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !uv pip install --no-deps unsloth\n",
        "\n",
        "# 💡 KEY LIBRARIES FOR FINE-TUNING:\n",
        "# - peft: Parameter-Efficient Fine-Tuning (LoRA implementation)\n",
        "# - trl: Transformer Reinforcement Learning (SFTTrainer for supervised training)\n",
        "# - xformers: Memory-efficient attention operations\n",
        "# - datasets: Hugging Face datasets library (loads emotion data)\n",
        "\n",
        "print(\"✅ Installation complete!\\n\")"
      ],
      "metadata": {
        "id": "3jJkIGJoCFG7",
        "outputId": "9564b4b2-782b-4f80-fefb-122c3662c462",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "📦 Installing Unsloth and Dependencies for Fine-Tuning\n",
            "================================================================================\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m8 packages\u001b[0m \u001b[2min 123ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m5 packages\u001b[0m \u001b[2min 1.98s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m5 packages\u001b[0m \u001b[2min 11ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mbitsandbytes\u001b[0m\u001b[2m==0.48.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcut-cross-entropy\u001b[0m\u001b[2m==25.1.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtrl\u001b[0m\u001b[2m==0.15.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1munsloth-zoo\u001b[0m\u001b[2m==2025.10.10\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mxformers\u001b[0m\u001b[2m==0.0.29.post3\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m5 packages\u001b[0m \u001b[2min 192ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 34ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 59ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1munsloth\u001b[0m\u001b[2m==2025.10.9\u001b[0m\n",
            "✅ Installation complete!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"🔍 Loading Base Model (Same as Baseline Test)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# Model configuration (same as baseline for fair comparison)\n",
        "max_seq_length = 2048  # Maximum context window\n",
        "dtype = None           # Auto-detect (FP16 for T4, BF16 for Ampere+)\n",
        "load_in_4bit = True    # 4-bit quantization to save memory\n",
        "\n",
        "# Load the same model used in baseline testing\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "# 💡 WHY SAME MODEL AS BASELINE?\n",
        "# We want to measure the impact of fine-tuning alone\n",
        "# By starting with the same model, we can directly compare:\n",
        "# - Baseline (no training) vs Fine-tuned (after training)\n",
        "\n",
        "print(f\"✅ Base model loaded: {model.config.model_type}\")\n",
        "print(f\"✅ Total parameters: ~3 Billion\")\n",
        "print(f\"✅ Memory: ~1.5-2 GB (4-bit quantized)\")\n",
        "\n",
        "print(\"-\"*80)\n",
        "print(\"=\"*80 + \"\\n\")"
      ],
      "metadata": {
        "id": "BGEaw_1tCHNk",
        "outputId": "e5ffa23a-dd05-4eb4-dabd-252756813093",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604,
          "referenced_widgets": [
            "af6ecfa0d3d847f79e2fc4b8515ac38e",
            "4e5baddd372b4e6981eb65432fae708a",
            "7240f801b5ed4a968d4859c55358c66b",
            "42df532d14204db0a47571356349b100",
            "1550be466e9e4802b3054ae7b321797a",
            "8b5b04b10ba24f388acffbc5bd947839",
            "8304a20af94e47019966943896de0f0f",
            "2074eee3290d41ba8b6c70befad0cbf1",
            "5f1903ba1b0f4203a2c7c81d11207430",
            "ad370236f51140299bace6801efdb976",
            "3d01d1489f4e4c13860ccbd4d7a71901"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "🔍 Loading Base Model (Same as Baseline Test)\n",
            "================================================================================\n",
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.6.0+cu124 with CUDA 1204 (you have 2.8.0+cu126)\n",
            "    Python  3.12.9 (you have 3.12.12)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========\n",
            "Switching to PyTorch attention since your Xformers is broken.\n",
            "========\n",
            "\n",
            "Unsloth: Xformers was not installed correctly.\n",
            "Please install xformers separately first.\n",
            "Then confirm if it's correctly installed by running:\n",
            "python -m xformers.info\n",
            "\n",
            "Longer error message:\n",
            "xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.6.0+cu124 with CUDA 1204 (you have 2.8.0+cu126)\n",
            "    Python  3.12.9 (you have 3.12.12)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.10.9: Fast Llama patching. Transformers: 4.57.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af6ecfa0d3d847f79e2fc4b8515ac38e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# 🎯 STEP 3: APPLY QLoRA (4-BIT BASE + LoRA ADAPTERS)\n",
        "#============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"🔧 Applying QLoRA: 4-bit Quantized Base + LoRA Adapters\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,  # LoRA rank: Higher = more capacity, more memory (8/16/32/64)\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",    # Attention layers\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",       # MLP layers\n",
        "    ],\n",
        "    lora_alpha = 64,              # LoRA scaling factor (typically 2× rank)\n",
        "    lora_dropout = 0,             # Dropout rate (0 is optimized for Unsloth)\n",
        "    bias = \"none\",                # Bias training (\"none\" is optimized)\n",
        "    use_gradient_checkpointing = \"unsloth\",  # Memory-efficient backprop\n",
        "    random_state = 3407,          # Random seed for reproducibility\n",
        "    use_rslora = False,           # Rank-Stabilized LoRA (not needed here)\n",
        "    loftq_config = None,          # LoftQ quantization (not needed)\n",
        ")\n",
        "\n",
        "# 💡 WHAT IS QLoRA?\n",
        "# QLoRA = Quantized LoRA, combining two techniques:\n",
        "#\n",
        "# 1. BASE MODEL: 4-bit NormalFloat (NF4) quantization\n",
        "#    - Frozen at 4-bit precision (~1.5GB memory)\n",
        "#    - Not trained, just used for forward pass\n",
        "#    - 75% memory reduction vs FP16 (1.5GB vs 6GB)\n",
        "#\n",
        "# 2. LoRA ADAPTERS: Low-Rank Adaptation in FP16/BF16\n",
        "#    - Small trainable matrices added to model\n",
        "#    - Original weight W (frozen in 4-bit)\n",
        "#    - LoRA adds: ΔW = A × B (small matrices in 16-bit)\n",
        "#    - New weight: W' = W + ΔW\n",
        "#    - Only A and B are trained (~0.5% of parameters, ~20-50MB)\n",
        "#\n",
        "# QLoRA Architecture:\n",
        "# ┌─────────────────────────────────────┐\n",
        "# │ Base Model (Frozen)                 │\n",
        "# │ - 4-bit NF4 quantization            │\n",
        "# │ - ~1.5 GB memory                    │\n",
        "# │ - Not updated during training       │\n",
        "# └─────────────────────────────────────┘\n",
        "#          ↓\n",
        "# ┌─────────────────────────────────────┐\n",
        "# │ LoRA Adapters (Trainable)           │\n",
        "# │ - FP16/BF16 precision               │\n",
        "# │ - ~20-50 MB memory                  │\n",
        "# │ - Updated during training           │\n",
        "# └─────────────────────────────────────┘\n",
        "#          ↓\n",
        "# Total: ~2GB memory (vs ~6GB for LoRA, ~12GB for full FP16 fine-tuning)\n",
        "#\n",
        "# Benefits:\n",
        "# - Extreme memory efficiency (train 3B on T4, 65B on A100)\n",
        "# - Fast training (fewer parameters to update)\n",
        "# - No quality loss vs regular LoRA (proven in paper)\n",
        "# - Easy to swap (keep base model, change adapters)\n",
        "# - High quality (95-99% of full fine-tuning performance)\n",
        "\n",
        "# 💡 RANK EXPLAINED:\n",
        "# Rank = 32 means each LoRA matrix has 32 dimensions\n",
        "# Higher rank = more capacity to learn, but more memory\n",
        "# - Rank 8: Fastest, lowest memory, good for simple tasks\n",
        "# - Rank 16: Balanced (common choice)\n",
        "# - Rank 32: Higher capacity, better for complex tasks (our choice)\n",
        "# - Rank 64+: Highest quality, but approaching full fine-tuning cost\n",
        "\n",
        "print(\"✅ QLoRA configuration:\")\n",
        "print(f\"   Base model: 4-bit NF4 quantization (~1.5GB, frozen)\")\n",
        "print(f\"   LoRA adapters: FP16/BF16 (~20-50MB, trainable)\")\n",
        "print(f\"   LoRA rank: 32 (higher capacity for better accuracy)\")\n",
        "print(f\"   LoRA alpha: 64 (2× rank, standard scaling)\")\n",
        "print(f\"   Target modules: 7 (Attention + MLP layers)\")\n",
        "print(\"\\n📊 Trainable Parameters:\")\n",
        "model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "id": "7CQbYNJzCQhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Dataset Link text](https://huggingface.co/datasets/dair-ai/emotion/viewer/split/train?views%5B%5D=split_train)**"
      ],
      "metadata": {
        "id": "uQj4fTKMSDMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# 📝 STEP 4: LOAD AND FORMAT DATASET\n",
        "#============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"📊 Loading Emotion Dataset from Hugging Face\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Define emotion labels (same as baseline)\n",
        "EMOTION_LABELS = {\n",
        "    0: \"sadness\",\n",
        "    1: \"joy\",\n",
        "    2: \"love\",\n",
        "    3: \"anger\",\n",
        "    4: \"fear\",\n",
        "    5: \"surprise\"\n",
        "}\n",
        "\n",
        "def to_llama3_format(example):\n",
        "    \"\"\"\n",
        "    Convert text-label pair to Llama 3.2 chat format.\n",
        "\n",
        "    Uses the tokenizer's apply_chat_template() for proper formatting.\n",
        "    This ensures the model sees data in the exact format it expects.\n",
        "    \"\"\"\n",
        "    text = example['text']\n",
        "    label = example['label']\n",
        "    emotion_name = EMOTION_LABELS[label]\n",
        "\n",
        "    # Create messages in chat format\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Identify the emotion in the following sentence and provide the emotion label.\"},\n",
        "        {\"role\": \"user\", \"content\": text},\n",
        "        {\"role\": \"assistant\", \"content\": f\"{label} ({emotion_name})\"}  # Expected output\n",
        "    ]\n",
        "\n",
        "    # Use tokenizer's built-in chat template\n",
        "    # add_generation_prompt=False because we include the assistant's response\n",
        "    formatted_text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False  # We have the full conversation\n",
        "    )\n",
        "\n",
        "    return {\"text\": formatted_text}\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"dair-ai/emotion\")\n",
        "\n",
        "# Use first 1000 samples for faster training (workshop demo)\n",
        "# For production, use full dataset: dataset['train']\n",
        "train_dataset = dataset['train'].select(range(1000)).map(\n",
        "    to_llama3_format,\n",
        "    remove_columns=['text', 'label']\n",
        ")\n",
        "\n",
        "print(f\"✅ Dataset: dair-ai/emotion\")\n",
        "print(f\"✅ Training samples: {len(train_dataset):,}\")\n",
        "print(f\"✅ Emotion classes: {len(EMOTION_LABELS)}\")\n",
        "print(f\"\\n📄 Sample formatted training example:\")\n",
        "print(\"-\" * 80)\n",
        "print(train_dataset[0][\"text\"][:300] + \"...\")\n",
        "print(\"-\" * 80 + \"\\n\")\n",
        "\n",
        "# 💡 WHY ONLY 1000 SAMPLES?\n",
        "# For workshop/demo purposes:\n",
        "# - Faster training (~10-15 min vs 30-60 min for full dataset)\n",
        "# - Still shows dramatic improvement over baseline\n",
        "# - For production: use full 16k training samples"
      ],
      "metadata": {
        "id": "Xc4-4WliCRez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# 🏋️ STEP 5: CONFIGURE TRAINING\n",
        "#============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"⚙️  Configuring Training Parameters\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    dataset_text_field = \"text\",       # Field name in dataset\n",
        "    max_seq_length = max_seq_length,   # Max tokens per example\n",
        "    dataset_num_proc = 2,               # Parallel data loading\n",
        "    packing = False,                    # Don't pack multiple examples together\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        # Batch configuration\n",
        "        per_device_train_batch_size = 2,   # Batch size per GPU\n",
        "        gradient_accumulation_steps = 4,   # Effective batch = 2 × 4 = 8\n",
        "\n",
        "        # Training duration\n",
        "        num_train_epochs = 2,               # Number of passes through data\n",
        "        # max_steps = 60,                   # Alternative: fixed number of steps\n",
        "\n",
        "        # Learning rate\n",
        "        learning_rate = 2e-4,               # How fast to learn\n",
        "        warmup_steps = 5,                   # Gradual learning rate warmup\n",
        "        lr_scheduler_type = \"cosine\",       # Learning rate decay schedule\n",
        "\n",
        "        # Optimization\n",
        "        optim = \"adamw_8bit\",               # Memory-efficient optimizer\n",
        "        weight_decay = 0.01,                # Regularization strength\n",
        "\n",
        "        # Precision (auto-detect based on GPU)\n",
        "        fp16 = not is_bfloat16_supported(), # Use FP16 on older GPUs (T4, V100)\n",
        "        bf16 = is_bfloat16_supported(),     # Use BF16 on newer GPUs (A100, A6000)\n",
        "\n",
        "        # Logging and saving\n",
        "        logging_steps = 1,                  # Log every step\n",
        "        output_dir = \"outputs\",             # Where to save checkpoints\n",
        "        report_to = \"none\",                 # Disable W&B/TensorBoard\n",
        "\n",
        "        # Reproducibility\n",
        "        seed = 3407,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# 💡 KEY TRAINING PARAMETERS EXPLAINED:\n",
        "#\n",
        "# Effective Batch Size = 2 × 4 = 8:\n",
        "#   - Real batch size: 2 (fits in memory)\n",
        "#   - Gradient accumulation: 4 (accumulate gradients from 4 batches)\n",
        "#   - Result: Same as training with batch size 8, but uses less memory\n",
        "#\n",
        "# Learning Rate = 2e-4:\n",
        "#   - Standard for LoRA fine-tuning\n",
        "#   - Lower than full fine-tuning (which uses 1e-5)\n",
        "#   - LoRA is more stable with higher learning rates\n",
        "#\n",
        "# Cosine Schedule:\n",
        "#   - Learning rate starts at 2e-4\n",
        "#   - Gradually decreases following cosine curve\n",
        "#   - Helps model converge smoothly\n",
        "#\n",
        "# 2 Epochs:\n",
        "#   - Model sees each of 1000 examples twice\n",
        "#   - Total steps: ~250 (1000 / 8 batch size × 2 epochs)\n",
        "#   - Training time: ~10-15 minutes on T4\n",
        "\n",
        "print(f\"✅ Effective batch size: {2 * 4}\")\n",
        "print(f\"✅ Training epochs: 2\")\n",
        "print(f\"✅ Learning rate: 2e-4 (with cosine decay)\")\n",
        "print(f\"✅ Optimizer: AdamW 8-bit (memory efficient)\")\n",
        "print(f\"✅ Expected training time: ~10-15 min on T4 GPU\")"
      ],
      "metadata": {
        "id": "UoSziI5kCVcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# 🚀 STEP 6: TRAIN THE MODEL\n",
        "#============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"🚀 Starting Fine-Tuning Training...\")\n",
        "print(\"=\"*80)\n",
        "print(\"Training on 1,000 emotion examples\")\n",
        "print(\"Watch the loss decrease - this shows the model is learning!\\n\")\n",
        "\n",
        "# Train!\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✅ Training Complete!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"📊 Final training loss: {trainer_stats.training_loss:.4f}\")\n",
        "print(f\"⏱️  Training time: {trainer_stats.metrics.get('train_runtime', 0):.1f} seconds\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# 💡 WHAT HAPPENED DURING TRAINING?\n",
        "# 1. Model processed 1000 emotion examples, 2 times (2 epochs)\n",
        "# 2. Learned to map text → emotion labels\n",
        "# 3. Learned the output format: \"0 (sadness)\", \"1 (joy)\", etc.\n",
        "# 4. Only LoRA adapters were trained (~0.5% of parameters, ~20-50MB)\n",
        "# 5. Base model weights remain frozen in 4-bit (QLoRA technique)\n",
        "# 6. Total memory usage: ~2GB (vs ~6GB for regular LoRA)\n"
      ],
      "metadata": {
        "id": "wf-05uMUCYmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# 🧪 STEP 7: TEST THE FINE-TUNED MODEL\n",
        "#============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"🧪 Testing Fine-Tuned Model\")\n",
        "print(\"=\"*80)\n",
        "print(\"Using SAME test sentences from baseline for fair comparison\\n\")\n",
        "\n",
        "# Enable inference mode (faster, no gradient calculation)\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "def predict_emotion(text):\n",
        "    \"\"\"\n",
        "    Predict emotion using the fine-tuned model.\n",
        "\n",
        "    Same function as baseline test, but now using trained model.\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Identify the emotion in the following sentence and provide the emotion label.\"},\n",
        "        {\"role\": \"user\", \"content\": text}\n",
        "    ]\n",
        "\n",
        "    # Format with chat template\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True  # Add <|assistant|> marker\n",
        "    )\n",
        "\n",
        "    # Tokenize and generate\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=20,\n",
        "        temperature=0.1,  # Low temperature for consistent classification\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "    # Decode and extract response\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    response = response.split(\"assistant\")[-1].strip()\n",
        "\n",
        "    return response\n",
        "\n",
        "# SAME test sentences as baseline (for comparison)\n",
        "test_sentences = [\n",
        "    \"i didnt feel humiliated\",\n",
        "    \"im grabbing a minute to post i feel greedy wrong\",\n",
        "    \"i am ever feeling nostalgic about the fireplace i will know that it is still on the property\",\n",
        "    \"i am feeling grouchy\",\n",
        "    \"ive been taking or milligrams or times recommended amount and ive fallen asleep a lot faster but i also feel like so funny\",\n",
        "    \"i feel as confused about life as a teenager or as jaded as a year old man\",\n",
        "    \"i need you i need someone i need to be protected and feel safe i am small now i find myself in a season of no words\"\n",
        "]\n",
        "\n",
        "print(\"Running predictions on test sentences...\")\n",
        "print(\"=\"*80 + \"\\n\")"
      ],
      "metadata": {
        "id": "4IpLThgACc9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test and display results\n",
        "results = []\n",
        "for i, sentence in enumerate(test_sentences, 1):\n",
        "    prediction = predict_emotion(sentence)\n",
        "    results.append({\n",
        "        \"input\": sentence,\n",
        "        \"output\": prediction\n",
        "    })\n",
        "    print(f\"[{i}/{len(test_sentences)}] {sentence[:60]}...\")\n",
        "    print(f\"→ {prediction}\\n\")\n",
        "\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "4my8yNIaChSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"📊 FINE-TUNING RESULTS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n✅ EXPECTED IMPROVEMENTS FROM BASELINE:\")\n",
        "print(\"   1. OUTPUT FORMAT: Now consistently follows '0 (sadness)' format\")\n",
        "print(\"   2. ACCURACY: 80-90%+ correct emotion identification\")\n",
        "print(\"   3. CONSISTENCY: Same input → same output (reproducible)\")\n",
        "\n",
        "print(\"\\n💡 WHAT THE MODEL LEARNED:\")\n",
        "print(\"   ✓ 6 emotion categories (sadness, joy, love, anger, fear, surprise)\")\n",
        "print(\"   ✓ Specific output format with number + name\")\n",
        "print(\"   ✓ Emotion patterns in text (keywords, context, sentiment)\")\n",
        "print(\"   ✓ Task-specific consistency\")\n",
        "\n",
        "print(\"\\n📈 TRAINING STATISTICS:\")\n",
        "print(f\"   Training samples: 1,000\")\n",
        "print(f\"   Epochs: 2\")\n",
        "print(f\"   Trainable parameters: ~0.5% of total (QLoRA)\")\n",
        "print(f\"   Memory usage: ~2GB (4-bit base + LoRA adapters)\")\n",
        "print(f\"   Training time: {trainer_stats.metrics.get('train_runtime', 0):.1f}s\")\n",
        "print(f\"   Final loss: {trainer_stats.training_loss:.4f}\")\n",
        "\n",
        "print(\"\\n🔍 COMPARE THESE RESULTS TO BASELINE:\")\n",
        "print(\"   Run llama4bit_pretraining.py to see the baseline (untrained)\")\n",
        "print(\"   You should see dramatic improvement in:\")\n",
        "print(\"   - Format adherence (was messy → now clean)\")\n",
        "print(\"   - Emotion accuracy (was random → now 80-90%+)\")\n",
        "print(\"   - Consistency (was varied → now deterministic)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✅ Fine-tuning demonstration complete!\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "================================================================================\n",
        "🎯 WORKSHOP FACILITATOR NOTES\n",
        "================================================================================\n",
        "\n",
        "1. BEFORE AND AFTER STORY:\n",
        "   - Show baseline results first (poor, inconsistent, wrong format)\n",
        "   - Run this training script (takes ~10-15 min)\n",
        "   - Show dramatic improvement on SAME test cases\n",
        "   - This visceral before/after is the key teaching moment\n",
        "\n",
        "2. QLoRA EFFICIENCY:\n",
        "   - QLoRA = 4-bit quantized base model + LoRA adapters\n",
        "   - Only 0.5% of parameters trained (LoRA adapters)\n",
        "   - Total memory: ~2GB (vs ~6GB for regular LoRA, ~12GB for full FP16)\n",
        "   - Adapters are tiny (~20-50 MB vs 6GB full model)\n",
        "   - Training is fast (10-15 min vs hours for full fine-tuning)\n",
        "   - Quality is 95-99% of full fine-tuning (no degradation vs LoRA)\n",
        "   - Can swap adapters: same base model, different tasks\n",
        "   - Breakthrough: Tim Dettmers' QLoRA paper (2023) enabled training 65B on single GPU\n",
        "\n",
        "3. KEY HYPERPARAMETERS:\n",
        "   - Rank 32: Higher than default (8/16) for better accuracy\n",
        "   - Learning rate 2e-4: Standard for LoRA (higher than full fine-tuning)\n",
        "   - Cosine schedule: Smooth learning rate decay\n",
        "   - 2 epochs: Enough for 1000 samples (more epochs on larger datasets)\n",
        "\n",
        "4. COMMON ISSUES:\n",
        "   - If loss doesn't decrease: Check data formatting\n",
        "   - If outputs still wrong: May need more epochs or data\n",
        "   - If CUDA OOM: Reduce batch size or sequence length\n",
        "   - If slow: Check GPU is being used (should be <1 min/epoch)\n",
        "\n",
        "5. PRODUCTION CONSIDERATIONS:\n",
        "   - Use full dataset (16k samples) not just 1000\n",
        "   - Add validation split to monitor overfitting\n",
        "   - Increase epochs to 3-5 for full dataset\n",
        "   - Save checkpoints periodically\n",
        "   - Test on held-out test set for final evaluation\n",
        "\n",
        "6. REAL-WORLD APPLICATIONS:\n",
        "   - Customer support: Classify ticket categories, urgency\n",
        "   - Content moderation: Detect toxic, spam, inappropriate\n",
        "   - Healthcare: Classify symptoms, triage severity\n",
        "   - Education: Grade sentiment in student feedback\n",
        "   - Any classification task with 100-10000 examples\n",
        "\n",
        "7. COST COMPARISON:\n",
        "   - Fine-tuning cost: $0.50-2 on Colab Pro (includes GPU time)\n",
        "   - API cost (no fine-tuning): $0.01 per 1k tokens × volume\n",
        "   - Crossover: If >50k-200k queries, fine-tuning cheaper\n",
        "   - Plus benefits: Privacy, control, customization\n",
        "\n",
        "================================================================================\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "G1uvf1u2CjsN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}